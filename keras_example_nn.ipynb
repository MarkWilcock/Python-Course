{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras example - build a neural net \n",
    "\n",
    "This lesson uses the MNIST digits dataset. This is a collection of 70,000 hand-written digits (0-9), with 60,000 for training and 10,000 for testing. Each digit is a 28x28 pixel grayscale image.  The MNIST dataset is a well known dataset in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import layers, models\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset is an internal dataset to the keras package.  The labels (y) are values between 0 and 9. Load the data into training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the first (of 60,000) training images.  It is a 28 x 28 array of values between 0 (black) and 255 (white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the image using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[y_train == 0] # returns all the zeros in the y_train\n",
    "y_train[y_train == 1] # returns all the ones in the y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some more of these images. The next code cell finds the first image of each class (0 through 9) in the training data and plots it with the label above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "# Create a subplot\n",
    "fig, ax = plt.subplots(1, num_classes, figsize=(20,20))  \n",
    "\n",
    "# Loop through 10 classes from train dataset and add labels from test dataset\n",
    "for i in range(num_classes): # 0 to 9\n",
    "  sample = X_train[y_train == i][0] # Get first image from each class\n",
    "  ax[i].imshow(sample, cmap=\"gray\") # Show sample image\n",
    "  ax[i].set_title(f\"Label:{i}\") # Set title as class label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to one-hot encode the classses so thatwe have a set of 10 0/1 vectors\n",
    "For example 2 becomes [0,0,1,0,0,0,0,0,0,0].\n",
    "\n",
    "One-hot encoding is a technique used in machine learning and data preprocessing to represent categorical data as binary vectors. It is particularly useful when dealing with categorical features or labels in a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    print(f\"Before : {y_train[i]}\")\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "for i in range(7):\n",
    "    print(f\"After : {y_train[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalise the data to ensure values are between 0 and 1.  This ensure that with several variable with different ranges, one variable does not dominate the ML calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0\n",
    "\n",
    "X_train[0] # now has values between 0 and 1 rather than 0 and 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image has 28 * 28 = 784values . Flatten the arrays to a single dimension 784 x 1 so that these can match the input layer of the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model.  This has two hideen layers.  The choice of units (nodes) in each layer in arbitary.  The output layer has 10 units corresponding to the 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=128, input_shape=(784, ), activation=\"relu\"))\n",
    "model.add(Dense(units=16, activation=\"relu\"))\n",
    "model.add(Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=\"accuracy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model against the training date.  This sets the weights between nodes at the bias for each node to mimise also function between the values and the training y values.  \n",
    "\n",
    "BATCH_SIZE and epochs control how long the proicess takes - these value are fairly arbitary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "epochs = 11\n",
    "model.fit(x=X_train, y=y_train, batch_size = BATCH_SIZE, epochs = epochs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how well the model does against the training and more imporatntly the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = model.evaluate(X_train, y_train)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print results\n",
    "print(f\"Train Loss: {train_loss}, Train Accuracy: {train_acc}\")\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows what the softmax algorithm does - chooses the biggest of the 10 probability values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1) # pick the highest probability with \"np.argmax()\", and turn it into an index uing \"axis=1\"\n",
    "\n",
    "# print vector of probabilities\n",
    "print(f\"What Softmax predicted: {y_pred}\")\n",
    "\n",
    "# print predicted number\n",
    "print(f\"What Softmax actually means: {y_pred_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_num = np.random.choice(len(X_test))\n",
    "X_sample = X_test[random_num]\n",
    "\n",
    "# save true label of this sample in a variable\n",
    "y_actual = np.argmax(y_test, axis=1)\n",
    "y_sample_actual = y_actual[random_num]\n",
    "\n",
    "# save a predicted label of this sample in a variable\n",
    "y_sample_pred_class = y_pred_classes[random_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(f\"random_num {random_num}  Predicted: {y_sample_pred_class}, True:{y_sample_actual}\")\n",
    "plt.imshow(X_sample.reshape(28, 28), cmap=\"gray\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
